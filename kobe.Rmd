---
title: "KobeShots"
output: html_notebook
---


In this report we are going to analyse a dataset containing a selection of Kobe Bryant's shots throughout his career. The goal of the analysis is to build a binary classification model that is able to predict the outcome of the shot given some informations about the shots, referred to as *features* or *predictors* from now on.

The dataset is publicy available in the CSV format and contains ca. 30K shots, described by means of 25 attributes.\newline
In the first section we are going to have a look at those attributes and perform a cleaning and transformation phase in order to make the dataset more suitable for further analysis. In the rest of the report we will use different techniques in the attempt of building a machine learning model that is able to correctly predict the \textit{shot\_made\_flag} attribute, which will be also called *target variable*.\newline
The analysis will be carried out using R 3.6 as well as different libraries, which will be pointed out each time.

### 1. Data preparation

#### 1.1 Exploration

The first phase in the analysis of a dataset must always be an exploration of the data provided. This may include a cleaning phase, in case some values or attributes are not needed, and a feature engineering phase, in which new predictors are created basing on the existing attributes and some kind of domain knowledge.\newline
After loading the dataset in the R environment, we proceed with a coarse data exploration and cleaning. The dataset contains 30.697 records, however many of them are missing some values and therefore we decide to prune them: this leaves us with 25.697 records.

```{r}
kobe <- read.csv(file = 'data.csv')
kobe=na.omit(kobe)
nrow(kobe)
```


Subsequently we have a look at the attributes provided in the dataset. We notice that some features are redundant: for example, *team_name* and *team_id* take on a single value, while *lat* and *lon* provide the same information as *loc_x* and *loc_y* on a different scale.\
It can also be useful to plot some of the attributes, in order to gain an intuitive feeling for the data we are dealing with. For example, we can plot loc_x and loc_y marking the successful shots in blue and the unsuccessful ones in red.


```{r}
attach(kobe)
dotsCol = rep("red", length(shot_made_flag))
dotsCol[which(shot_made_flag==TRUE)] = "blue"
plot(loc_y, loc_x, type="p", cex=0.1, pch=20, col=dotsCol)
```


### 1.2 Feature engineering
Using some domain knowledge - e.g. that an NBA court is 94 by 50 feet - we can imply that loc_x and loc_y are expressed in tenth (1/10 of a foot). Since many of the analyses we are going to present base on some kind of linearity between the predictors and the target variable we prefer not to have a big differentiation between similar shots, such as those taken from the right angle and from the left angle, which have comparable rate of success.\
Therefore we add two new features, called *feet_x* and *feet_y*, which express the absolute value of the original attributes, divided by 10 (i.e. converted into feet). We create also an additional value, *shot_side*, which signals whether the shot was taken from the right side (i.e. in the positive range of loc_x) or from the left side. In this way it will be easier to point out in the following sections if the side does play a role in the outcome of the shot or not.

Another example of feature engineering is the use of different coordinate systems to express the shot location. Along with the cartesian system used in feet_x and feet_y, here we want to also employ the polar coordinates, which consider the shot distance and the angle between the shooter and the rim. In fact, even though there is a one-to-one mapping between the two systems, this is not achieved through a linear function: for this reason one of them could e.g. better fit the linear relationship in the log-odds of the prediction, which would reduce the bias of the logistic regression.\
Furthermore, we consider the attribute *season*, which is qualitative in the original dataset. Since we expect to see a gradual evolution in Bryant's shots over his career, we rather prefer to consider this variable as numerical, so that we the model can integrate more easily the temporal informations in its prediction.\
We conclude the first phase looking at the attribute *shot_type*, which is a categorical attribute with as many as 58 levels. However, many of these levels appear only a few times in the dataset, which makes the analysis more difficult and less generalizable. Therefore we group all the levels with less than 50 instances in a new level called \"Other\".


```{r}
# create coordinates systems
feet_x = abs(loc_x)/10
feet_y = loc_y/10
kobe$shot_distance = sqrt(feet_x^2 + feet_y^2) # distance in the polar coordinates
shot_angle = atan2(feet_y, feet_x)             # angle in the polar coordinates
shot_side = ifelse(loc_x >= 0, "Right", "Left")
shot_side <- as.factor(shot_side)
kobe = cbind(kobe, shot_angle, feet_x, feet_y, shot_side)
# make season continuous
season = substr(season, 1, 4)
season = strtoi(season)
kobe$season = season
# create new factor in action_type
rareVars = names(which(summary(action_type) < 50))
rareIndex = action_type %in% rareVars
kobe$action_type = factor(kobe$action_type, levels=c(levels(action_type), "Other"))
kobe$action_type[rareIndex] = "Other"
action_type = kobe$action_type
# prune unuseful attributes
kobe = kobe[,-which(names(kobe) %in% c("minutes_remaining","seconds_remaining","team_name", "team_id", "matchup", "shot_zone_range", "game_event_id", "game_id", "game_date", "shot_id", "lat", "lon", "loc_x", "loc_y"))]
```


```{r}

```

### 2. Logistic regression
The first analysis we are going to perform is called logistic regression. This algorithm assigns a value between 0 and 1 to each sample and can be used to build a binary classifier, whereby the output value is interpreted as the probability of the sample belonging to one of the two classes.\
Formally the output value is obtained through the formula

<center>$y=p(x)=\frac{1}{1+exp(xw)}$</center>

where *x* is the input vector and *w* is a vector of parameters. The goal of the training phase is hence to finetune w so as to maximize the probability that the samples are assigned to the right class, which is done usually through gradient descent.\
Even though the above formula is clearly non-linear, it can be seen though some easy transformations that logistic regression delivers a linear decision boundary, and that the log-odds (i.e. the logarithm of the odds $\frac{p(x)}{1-p(x)}$) are linear in x.

#### 2.1 First model
We start by building a first, naive logistic regression model. First, we have which predictors we use to train our model: at this points, it's important not to include correlated attributes in the predictors. It is important to remove strong correlations among the attributes since they can negative impact: in particular, they can increase the variance of the coefficients and hence harm the reliability of the result.\
We can check the correlation among continuous variables building a correlation matrix:

```{r}
#cor(kobe[,-c(1,7,8,9,14)])
```

Here we see that most of the variables are not strongly correlated. This does not hold for feet_x, feet_y, shot_distance and shot_angle. This is no wonder, since shot_distance and shot_angle have been created starting from the other two.\
Similarly, *combined_shot_sype* is a generalization of *shot_type*, while *shot_zone_area*, *shot_zone_basic* and *shot_zone_range* are a generalization of the shot coordinates, therefore we ignore them for now.

```{r}
logreg = glm(shot_made_flag~.-combined_shot_type-shot_zone_area-shot_zone_basic-feet_x-feet_y , family=binomial, data=kobe)
summary(logreg)
```

We can see that in order to build the model all categorical predictors have to be transformed into dummy variables. In the case of binary predictors, such as shot_side, this simply means marking one value (in this case "Right") with 1 and the other one with 0. In the case of larger domains, there has to be one dummy variable for every possible value (minus one), which is 1 if the predictor takes on that value. Accordingly, the related coefficient indicates how the output changes when the attribute takes on that value.\
We can have an insight on the statistical significance of our prediction by looking at the z-value and the corresponding p-value (displayed in the script as *Pr(>|z|)*). The z-value is a score that says how far each coefficient is from 0, which in turn would imply there is no relationship between the predictor and the response. The z-value is computed using the sample standard error: we can then look at the p-value to consider how likely is that the coefficient is 0, given its z-value.\
In this first model, we can see that the dummy variables derived from opponent have quite high p-values, which suggests that the outcome of a shot does not depend on the opposing team. On the contrary, the coefficient of shot_angle is very unlikely to be 0, even though it has a small absolute value (0.12): that is because its standard error is so little (0.03) that the z-score is cranked up, so as we can rely on the statistical significance of the coefficient. If we consider how shot_angle was constructed, we see that its value is 0 for shots taken from the base line and $\pi$/2 for shots taken face the rim. Therefore, the value of 0.12 means that shots taken in front of the rim have higher odds of being successful, since they have higher values of shot_angle.\

#### 2.2 Evaluation

##### 2.2.1 Validation set
The simplest way to evaluate a model is to use validation set. This approach consists in setting aside a part of the samples during the construction of the model and using them only in the evaluation. Common size proportions between training and validation set are 70%-30% and 80%-20%.\
Clearly, in order to evaluate the models from the last section we have to repeat also the training phase, because we are not allowed to use the samples in the validation phase for building the model. However, we expect that the results obtained sofar also hold for the new training set, since its samples are chosen randomly.\
Doing so, we can 

```{r}
set.seed(0)
# Build training and validation set
train_id = sample(nrow(kobe), 0.7*nrow(kobe))
test_x = kobe[-train_id,]
test_y = shot_made_flag[-train_id]
# Rebuild logistic regression models
logreg1 = glm(shot_made_flag~.-combined_shot_type-shot_zone_area-shot_zone_basic-feet_x-feet_y , family=binomial, data=kobe, subset=train_id)
# Evaluate models
probs1 = predict(logreg1, test_x, type="response")
preds1 = ifelse(probs1>.45, 1, 0)
acc1 = sum(preds1==test_y)/length(test_y)
```


##### 2.2.2 Cross-validation
```{r}
# TODO: review..is it correct?
library(boot)
costFun <- function(r, pi) mean(abs(r-pi)> 0.5)
logreg1 = glm(shot_made_flag~.-shot_zone_area-shot_zone_basic-feet_x-feet_y , family=binomial, data=kobe)
cv1 = cv.glm(kobe, logreg1, cost=costFun, K=10)$delta[1]
logreg2 = glm(shot_made_flag~.-shot_zone_area-shot_zone_basic-feet_x-feet_y-playoffs-season-opponent-shot_side, family=binomial, data=kobe)
cv2 = cv.glm(kobe, logreg2, cost=costFun, K=10)$delta[1]
```

#### 2.3 Improving the model
The presence of insignificant predictors can negatively impact on the model, for example by increasing its variance. Therefore we now refine our first model by ignoring such predictors; in the next section we will evaluate both models and see whether this decision leads to a better performance.

```{r}
# 1. ignore attributes with high p-value
logreg2 = glm(shot_made_flag~.-combined_shot_type-shot_zone_area-shot_zone_basic-feet_x-feet_y-playoffs-season-shot_distance-opponent-shot_side, family=binomial, data=kobe, subset=train_id)
probs2 = predict(logreg2, test_x, type="response")
preds2 = ifelse(probs2>.45, 1, 0)
acc2 = sum(preds2==test_y)/length(test_y)
```

```{r}
# 2. cartesian coordinates
logreg3 = glm(shot_made_flag~.-combined_shot_type-shot_zone_area-shot_zone_basic-shot_angle-shot_distance-playoffs-season-shot_side, family=binomial, data=kobe, subset=train_id)
probs3 = predict(logreg3, test_x, type="response")
preds3 = ifelse(probs3>.45, 1, 0)
acc3 = sum(preds3==test_y)/length(test_y)
```

```{r}
# model variance
logreg4 = glm(shot_made_flag~.-action_type-shot_zone_area-shot_zone_basic-feet_x-feet_y , family=binomial, data=kobe, subset=train_id)
coef(summary(logreg1))[,4][27:31]
coef(summary(logreg4))[,4][7:11]
probs4 = predict(logreg4, test_x, type="response")
preds4 = ifelse(probs4>.45, 1, 0)
acc4 = sum(preds4==test_y)/length(test_y)
```

### 3. Decision tree
A simple, yet effective method to perform statistical learning analysis is to grow a decision tree. This technique consists in building a set of rules that predict the output in a step-by-step, greedy way. Such rules are arranged in a tree structure, whose internal nodes are boolean conditions on the attibutes that lead to a unique prediction for each input sample.

A central point in the construction of a decision tree is the choice of the attributes that are taken into account at each step. There are different criteria to solve this problems, the most popular ones for classification purposes are the *entropy* and the *Gini index*.\
In this context we are going to focus on the entropy (or *deviance*), which is expressed by the formula:

<center>$E=-\sum_{k=1}^{K} p_k*log(p_k)$</center>

where $p_k$ represents the relative frequency of a class in a given set of samples. During the construction of the model, we want to choose those attributes that allow to split the original dataset in such a way that E is minimized.\
The tree can be then grown until there are no more attributes or we can set a stop rule, regarding mostly the number of steps or the minimal improvement obtained through a new step.

The standard R implementation of the algorithm forces us to discard in advance those factors (i.e. categorical variables) that have more than 32 levels in the domain, which in our case are opponent and action_type.\
As we run the algorithm we notice that only one attributes is considered, namely combined_shot_type. Nevertheless, the validation accuracy is roughly 61%

```{r}
library(tree)
shot_made_bool = ifelse(shot_made_flag==1, "Yes", "No")
kobe_frame = data.frame(kobe, shot_made_bool)
kobe_frame = kobe_frame[,-7]
tree = tree(shot_made_bool~.-opponent-action_type, kobe_frame,  split="deviance", subset=train_id)
preds = predict(tree, kobe_frame[-train_id,], type="class")
plot(tree)
text(tree, pretty=0)
```



### 3.1 Bagging
While decision trees are easily interpretable and produce a decent accuracy, they turn out to be highly instable, since little modifications in the dataset produce big modifications in the model. This results in high variance and hence in overfitting, which can harm the quality of the result when dealing with new samples.\
One possible workaround is to build several trees, starting from different datasets, in order to reduce the impact of a single modification on the final result. The technique called *Bootstrap aggregation*, also called *bagging*, follows this approach, whereby the different datasets are in fact the result of multiple samplings with replacement from the original dataset.\
In this way we build a certain number of trees that can be possibly very different from each other, due to the high variance of the decision tree algorithm. The prediction is then obtained through a majority vote among all the trees, so that a slight modification in one or a few trees does not necessarily influence the outcome.\
The numbers of trees is a hyperparameter of the technique, that can be tuned using the typical validation techniques (validation set or cross-validation); standard values are in the order of several hundreds. Clearly, using so many models in parallel comes at a cost both when it comes to train the model and to produce a prediction.\



<!-- Try hyperparameter nr. trees-->
```{r}
library(randomForest)
set.seed(0)
acc= rep(NA, 10)
for(i in 1:1) {
  bag = randomForest(shot_made_bool~.-action_type-shot_made_flag,data=kobe_frame, subset=train_id, mtry=13, importance=TRUE)
  preds = predict(bag, newdata=kobe[-train_id,])
  preds = ifelse(preds=="No", 0, 1)
  acc = sum(preds==test_y)/length(test_y) 
}


```


### 3.1 Random forest
```{r}
set.seed(0)
forest = randomForest(shot_made_bool~.-action_type-shot_made_flag,data=kobe_frame, subset=train_id, importance=TRUE)
preds = predict(forest, newdata=kobe[-train_id,])
table(preds, test_y)
```

### 5. Svm
```{r}
library(e1071)
dat = data.frame(x=kobe[,-7], y=as.factor(shot_made_flag))
svmfit = svm(shot_made_flag~., data=dat, kernel="linear", cost=10, scale=FALSE)
testdat = data.frame(x=kobe[-train_id,], y=as.factor(test_y))
preds = predict(svmfit, testdat)
table(predict=preds, truth=testdat$y)
```



### 6. Knn
<!-- We use only continuous (season, feet_x, feet_y, distance, angle) and ordinal () variables -->
<!-- Use different distances -->
```{r}
library(class)
set.seed(1)
shot_type_num = ifelse(shot_type=="3PT Field Goal", 1, 0)
train.X = cbind(season, feet_x, feet_y, shot_distance, shot_angle, shot_type_num)[train_id,]
train.Y = shot_made_flag[train_id]
test.X = cbind(season, feet_x, feet_y, shot_distance, shot_angle, shot_type_num)[-train_id,]
preds = knn(train.X, test.X, train.Y, k=5)
table(preds, test_y)
acc = sum(preds==test_y)/length(test_y)
```

### 7. Dimensionality reduction

#### 7.1 Factor Analysis of Mixed Data (FAMD)
```{r}
library("FactoMineR")
library("factoextra")
famd = FAMD(kobe[,-which(names(
  kobe)=="shot_made_flag")], graph = FALSE)
get_eigenvalue(famd)
fviz_screeplot(famd)
var = get_famd_var(famd)
var$contrib
fviz_famd_var(famd, repel = TRUE)
```


##### 7.2 Pca
```{r}
pr.out = prcomp(kobe, scale=TRUE)
```


### (7. Evaluation metrics)
<!--
hyperparameters (logistic regression threshold)
other metrics (precision, recall, roc curve)
-->

<!--
Compare all models
-->