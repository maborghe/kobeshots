---
title: "KobeShots"
output: html_notebook
---


In this report we are going to analyse a dataset containing a selection of Kobe Bryant's shots throughout his career. The goal of the analysis is to build a binary classification model that is able to predict the outcome of the shot given some informations about the shots, referred to as *features* or *predictors* from now on.

The dataset is publicy available in the CSV format and contains ca. 30K shots, described by means of 25 attributes.\newline
In the first section we are going to have a look at those attributes and perform a cleaning and transformation phase in order to make the dataset more suitable for further analysis. In the rest of the report we will use different techniques in the attempt of building a machine learning model that is able to correctly predict the \textit{shot\_made\_flag} attribute, which will be also called *target variable*.\newline
The analysis will be carried out using R 3.6 as well as different libraries, which will be pointed out each time.

### 1. Data preparation

#### 1.1 Exploration

The first phase in the analysis of a dataset must always be an exploration of the data provided. This may include a cleaning phase, in case some values or attributes are not needed, and a feature engineering phase, in which new predictors are created basing on the existing attributes and some kind of domain knowledge.\newline
After loading the dataset in the R environment, we proceed with a coarse data exploration and cleaning. The dataset contains 30.697 records, however many of them are missing some values and therefore we decide to prune them: this leaves us with 25.697 records.

```{r}
kobe <- read.csv(file = 'data.csv')
kobe=na.omit(kobe)
nrow(kobe)
```


Subsequently we have a look at the attributes provided in the dataset. We notice that some features are redundant: for example, *team_name* and *team_id* take on a single value, while *lat* and *lon* provide the same information as *loc_x* and *loc_y* on a different scale.\
It can also be useful to plot some of the attributes, in order to gain an intuitive feeling for the data we are dealing with. For example, we can plot loc_x and loc_y marking the successful shots in blue and the unsuccessful ones in red.

<!-- TODO: class distribution!! -->


```{r}
attach(kobe)
dotsCol = rep("red", length(shot_made_flag))
dotsCol[which(shot_made_flag==TRUE)] = "blue"
plot(loc_y, loc_x, type="p", cex=0.1, pch=20, col=dotsCol)
```


### 1.2 Feature engineering
Using some domain knowledge - e.g. that an NBA court is 94 by 50 feet - we can imply that loc_x and loc_y are expressed in tenth (1/10 of a foot). Since many of the analyses we are going to present base on some kind of linearity between the predictors and the target variable we prefer not to have a big differentiation between similar shots, such as those taken from the right angle and from the left angle, which have comparable rate of success.\
Therefore we add two new features, called *feet_x* and *feet_y*, which express the absolute value of the original attributes, divided by 10 (i.e. converted into feet). We create also an additional value, *shot_side*, which signals whether the shot was taken from the right side (i.e. in the positive range of loc_x) or from the left side. In this way it will be easier to point out in the following sections if the side does play a role in the outcome of the shot or not.

Another example of feature engineering is the use of different coordinate systems to express the shot location. Along with the cartesian system used in feet_x and feet_y, here we want to also employ the polar coordinates, which consider the shot distance and the angle between the shooter and the rim. In fact, even though there is a one-to-one mapping between the two systems, this is not achieved through a linear function: for this reason one of them could e.g. better fit the linear relationship in the log-odds of the prediction, which would reduce the bias of the logistic regression.\
Furthermore, we consider the attribute *season*, which is qualitative in the original dataset. Since we expect to see a gradual evolution in Bryant's shots over his career, we rather prefer to consider this variable as numerical, so that we the model can integrate more easily the temporal informations in its prediction.\
We conclude the first phase looking at the attribute *shot_type*, which is a categorical attribute with as many as 58 levels. However, many of these levels appear only a few times in the dataset, which makes the analysis more difficult and less generalizable. Therefore we group all the levels with less than 50 instances in a new level called \"Other\".


```{r}
# create coordinates systems
feet_x = abs(loc_x)/10
feet_y = loc_y/10
kobe$shot_distance = sqrt(feet_x^2 + feet_y^2) # distance in the polar coordinates
shot_angle = atan2(feet_y, feet_x)             # angle in the polar coordinates
shot_side = ifelse(loc_x >= 0, "Right", "Left")
shot_side <- as.factor(shot_side)
kobe = cbind(kobe, shot_angle, feet_x, feet_y, shot_side)
# make season continuous
season = substr(season, 1, 4)
season = strtoi(season)
kobe$season = season
# create new factor in action_type
rareVars = names(which(summary(action_type) < 50))
rareIndex = action_type %in% rareVars
kobe$action_type = factor(kobe$action_type, levels=c(levels(action_type), "Other"))
kobe$action_type[rareIndex] = "Other"
action_type = kobe$action_type
# prune unuseful attributes
kobe = kobe[,-which(names(kobe) %in% c("minutes_remaining","seconds_remaining","team_name", "team_id", "matchup", "shot_zone_range", "game_event_id", "game_id", "game_date", "shot_id", "lat", "lon", "loc_x", "loc_y"))]
```

### 2. Logistic regression
The first analysis we are going to perform is called logistic regression. This algorithm assigns a value between 0 and 1 to each sample and can be used to build a binary classifier, whereby the output value is interpreted as the probability of the sample belonging to one of the two classes.\
Formally the output value is obtained through the formula

<center>$y=p(x)=\frac{1}{1+exp(xw)}$</center>

where *x* is the input vector and *w* is a vector of parameters. The goal of the training phase is hence to finetune w so as to maximize the probability that the samples are assigned to the right class, which is done usually through gradient descent.\
Even though the above formula is clearly non-linear, it can be seen though some easy transformations that logistic regression delivers a linear decision boundary, and that the log-odds (i.e. the logarithm of the odds $\frac{p(x)}{1-p(x)}$) are linear in x.

#### 2.1 First model
We start by building a first, naive logistic regression model. First, we have which predictors we use to train our model: at this points, it's important not to include correlated attributes in the predictors. It is important to remove strong correlations among the attributes since they can negative impact: in particular, they can increase the variance of the coefficients and hence harm the reliability of the result.\
We can check the correlation among continuous variables building a correlation matrix:

```{r}
#cor(kobe[,-c(1,7,8,9,14)])
```

Here we see that most of the variables are not strongly correlated. This does not hold for feet_x, feet_y, shot_distance and shot_angle. This is no wonder, since shot_distance and shot_angle have been created starting from the other two.\
Similarly, *combined_shot_sype* is a generalization of *shot_type*, while *shot_zone_area*, *shot_zone_basic* and *shot_zone_range* are a generalization of the shot coordinates, therefore we ignore them for now.

```{r}
logreg = glm(shot_made_flag~.-combined_shot_type-shot_zone_area-shot_zone_basic-feet_x-feet_y, family=binomial, data=kobe)
summary(logreg)
```

We can see that in order to build the model all categorical predictors have to be transformed into dummy variables. In the case of binary predictors, such as shot_side, this simply means marking one value (in this case "Right") with 1 and the other one with 0. In the case of larger domains, there has to be one dummy variable for every possible value (minus one), which is 1 if the predictor takes on that value. Accordingly, the related coefficient indicates how the output changes when the attribute takes on that value.\
We can have an insight on the statistical significance of our prediction by looking at the z-value and the corresponding p-value (displayed in the script as *Pr(>|z|)*). The z-value is a score that says how far each coefficient is from 0, which in turn would imply there is no relationship between the predictor and the response. The z-value is computed using the sample standard error: we can then look at the p-value to consider how likely is that the coefficient is 0, given its z-value.\
In this first model, we can see that the dummy variables derived from opponent have quite high p-values, which suggests that the outcome of a shot does not depend on the opposing team. On the contrary, the coefficient of shot_angle is very unlikely to be 0, even though it has a small absolute value (0.12): that is because its standard error is so little (0.03) that the z-score is cranked up, so as we can rely on the statistical significance of the coefficient. If we consider how shot_angle was constructed, we see that its value is 0 for shots taken from the base line and $\pi$/2 for shots taken face the rim. Therefore, the value of 0.12 means that shots taken in front of the rim have higher odds of being successful, since they have higher values of shot_angle.\

#### 2.2 Evaluation

##### 2.2.1 Validation set
The simplest way to evaluate a model is to use a validation set. This approach consists in setting aside a part of the samples during the construction of the model and using them only in the evaluation. Common size proportions between training and validation set are 70%-30% and 80%-20%.\
Clearly, in order to evaluate the models from the last section we have to repeat also the training phase, because we are not allowed to use the samples in the validation phase for building the model. However, we expect that the results obtained sofar also hold for the new training set, since its samples are chosen randomly.\
As we have said before, the output of a logistic regression model is a value between 0 and 1, which can be interpreted as the probability of the sample belonging to one class. However, when it comes to mark a class label we have to choose the threshold above which samples are assigned to class \"1\". This can vary according to our needs: for example, if we want to avoid false negatives in particular we might set a threshold lower than 0.5.\
In our case, the two classes are quite balanced and missclassification has the same cost in both cases, so we can expect a 50% threshold to work well. Indeed, we see in the plot that the maximum accuracy is reached at 0.5 and it amounts to 68.2%.

```{r}
set.seed(0)
# Build training and validation set
train_id = sample(nrow(kobe), 0.7*nrow(kobe))
test_x = kobe[-train_id,]
test_y = shot_made_flag[-train_id]
# Rebuild and evaluate model
logreg1 = glm(shot_made_flag~.-combined_shot_type-shot_zone_area-shot_zone_basic-feet_x-feet_y , family=binomial, data=kobe, subset=train_id)
probs1 = predict(logreg1, test_x, type="response")
acc= rep(NA, 9)
thresholds = c(0.2, 0.3, 0.4, 0.45, 0.5, 0.55, 0.6, 0.7, 0.8)
for (i in 1:9) {
  preds1 = ifelse(probs1>thresholds[i], 1, 0)
  acc[i] = sum(preds1==test_y)/length(test_y)  
}
plot(thresholds, acc, xlab="threshold", ylab="accuracy", pch=20, col="blue")
```


##### 2.2.2 Cross-validation
An alternative approach for evaluating a model is to use cross-validation. This technique includes dividing the data set in N buckets and building N different models, each using all buckets but one as training set (specifically, the i-th model uses buckets but the i-th one). The remaining bucket is then used as validation set, the final accuracy is then computed as the average accuracy of all the models.\
This technique is particularly suited for small datasets, where using a significant percentage of the samples solely for validation can impact on the stability of the models: in fact, many coefficient measures are directly related to the number of samples used, hence having less samples increases the standard error and the p-values, makes the coefficients intervals wider, and so on.\
The disadvantage of this technique lies in higher computational costs, since it requires to build several models (typical values for N are 5 and 10). In our case we have sufficient samples to use a separated validation set and perform cross validation only in this section. In particular we perform 10-fold CV on the logistic regression model seen in section 2.1: this results in an accuracy of 68.1%, which is basically the same as we obtained with the validation set (the threshold here is implicitly set to 0.5).
```{r}
library(boot)
costFun <- function(r, pi) mean(abs(r-pi)> 0.5)
cv_err = cv.glm(kobe, logreg, cost=costFun, K=10)$delta[1] # CV error
1-cv_err # accuracy
```

#### 2.3 Improving the model
The presence of insignificant predictors can negatively impact on a model, for example by increasing its variance. Therefore we now refine the first model built in section 2.1 by excluding those attributes that show a high p-value, i.e. a high probability that their true coefficient is 0 and hence they are not related to the target variable. These attributes are *playoffs*, *opponent*, *shot_side* and *shot_distance*. We evaluate the new model with with the help of the validation set and obtain an accuracy of 68.1%, which turns out to be the same as before.

```{r}
logreg2 = glm(shot_made_flag~.-combined_shot_type-shot_zone_area-shot_zone_basic-feet_x-feet_y-playoffs-shot_distance-opponent-shot_side, family=binomial, data=kobe, subset=train_id)
probs2 = predict(logreg2, test_x, type="response")
preds2 = ifelse(probs2>.5, 1, 0)
sum(preds2==test_y)/length(test_y) # accuracy
```

Another possibility to transform the initial model is to use the cartesian coordinates feet_x and feet_y instead of the polar ones, i.e. shot_angle and shot_distance. Also here, the validation accuracy remains stable: this could be partially already expected by the fact the shot_distance had already been proved to be statistically irrelevant in the prediction.\
We can also try adding an interacion term feet_x*feet_y, basing on the consideration that shots with a high value in both coordinates are frequently more difficult than 3 pointers taken from the angle or facing the rim, which display a high value only in one of the coordinates. However, also this does not improve the validation accuracy, and the interaction term indeed turns out to have a p-value of 0.06, which makes it unnecessary.

```{r}
logreg3 = glm(shot_made_flag~.-combined_shot_type-shot_zone_area-shot_zone_basic-shot_angle-shot_distance, family=binomial, data=kobe, subset=train_id)
#logreg3 = glm(shot_made_flag~.-combined_shot_type-shot_zone_area-shot_zone_basic-shot_angle-shot_distance+feet_x*feet_y, family=binomial, data=kobe, subset=train_id)
probs3 = predict(logreg3, test_x, type="response")
preds3 = ifelse(probs3>.5, 1, 0)
sum(preds3==test_y)/length(test_y)
```

We conclude this section about logistic regression showing some limitations of the z-score and the related p-value. As we have seen sofar, these values express the statistical significance of a predictor; in particular the p-value can be interpreted as the likelihood of the true coefficient being zero, given the estimated coefficient and the sample standard error.\
However, as we already said this value can be significantly influenced by other factors, e.g. the presence of correlated attributes, which we are not going to analyse further in this report. Instead we will give here a more general example of how the p-value of one attribute can be considerably impacted by the presence of certain predictors.\
To do that we build a new model which does not contain the action_type attribute, but instead takes in the *combined_shot_type*, which is just a generalization of the former one. We then look at the p-values of other attributes in the model. As a consequence of the lack of information, the validation accuracy drops suddenly to 60.8%, which points out the importance of the action_type attribute with respect to most of the other ones, which had little or no impact at test time, as we have seen in the last sections.\
At the same time, however, we also see that the degree of rejection of the null hypothesis for the other predictors changes significantly: for example the p-value for *season* is reduced by a factor $10^4$, while for *shot_distance* it increases by $10^3$.

```{r}
# model variance
logreg4 = glm(shot_made_flag~.-action_type-shot_zone_area-shot_zone_basic-feet_x-feet_y , family=binomial, data=kobe, subset=train_id)
pvalues1 = coef(summary(logreg1))[,4][27:31]
pvalues2 = coef(summary(logreg4))[,4][7:11]
tab = matrix(c(pvalues1, pvalues2), ncol=2, byrow=TRUE)
colnames(tab) = c("logreg1", "logreg4")
rownames(tab) = c("period", "playoffs", "season", "shot_distance", "shot_type")
tab
#probs4 = predict(logreg4, test_x, type="response")
#preds4 = ifelse(probs4>.45, 1, 0)
#sum(preds4==test_y)/length(test_y)

```

### 3. Decision tree
A simple, yet effective method to perform statistical learning analysis is to grow a decision tree. This technique consists in building a set of rules that predict the output in a step-by-step, greedy way. Such rules are arranged in a tree structure, whose internal nodes are boolean conditions on the attibutes that lead to a unique prediction for each input sample.

A central point in the construction of a decision tree is the choice of the attributes that are taken into account at each step. There are different criteria to solve this problems, the most popular ones for classification purposes are the *entropy* and the *Gini index*.\
In this context we are going to focus on the entropy (or *deviance*), which is expressed by the formula:

<center>$E=-\sum_{k=1}^{K} p_k*log(p_k)$</center>

where $p_k$ represents the relative frequency of a class in a given set of samples. During the construction of the model, we want to choose those attributes that allow to split the original dataset in such a way that E is minimized.\
The tree can be then grown until there are no more attributes or we can set a stop rule, regarding mostly the number of steps or the minimal improvement obtained through a new step.

The standard R implementation of the algorithm forces us to discard in advance those factors (i.e. categorical variables) that have more than 32 levels in the domain, which in our case are opponent and action_type.\
As we run the algorithm we notice that only one attributes is considered, namely combined_shot_type. Nevertheless, the validation accuracy is roughly 61%

```{r}
library(tree)
shot_made_bool = ifelse(shot_made_flag==1, "Yes", "No")
kobe_frame = data.frame(kobe, shot_made_bool)
kobe_frame = kobe_frame[,-7]
tree = tree(shot_made_bool~.-opponent-action_type, kobe_frame,  split="deviance", subset=train_id)
preds = predict(tree, kobe_frame[-train_id,], type="class")
plot(tree)
text(tree, pretty=0)
```



### 3.1 Bagging
While decision trees are easily interpretable and produce a decent accuracy, they turn out to be highly instable, since little modifications in the dataset can produce big changes in the model. This results in high variance and hence in overfitting, which can harm the quality of the result when dealing with new samples.\
One possible workaround is to build several trees, starting from different datasets, in order to reduce the impact of a single modification on the final result. The technique called *Bootstrap aggregation*, also called *bagging*, follows this approach, whereby the different datasets are in fact the result of multiple samplings with replacement from the same dataset.\
In this way we build a certain number of trees that can be possibly very different from each other, due to the high variance of the decision tree algorithm. The prediction is then obtained through a majority vote among all the trees, so that a slight modification in one or a few trees does not necessarily influence the outcome.\
The numbers of trees is a hyperparameter of the technique, that can be tuned using the typical validation techniques (validation set or cross-validation); typical values are in the order of several hundreds. Clearly, using so many models in parallel comes at a cost both when it comes to train the model and to produce a prediction.\
We train different models with a number of trees between 200 and 800, however the validation accuracy does not exceed 58%, which is worse than the single decision tree. This may be due to implementation details of the two algorithms, but for us it means primarily that the model using the decision tree does not suffer from excessive variance and overfitting.

```{r}
library(randomForest)
acc = rep(NA, 7)
ntrees = seq(200, 800, 100)
for(i in 1:7) {
  bag = randomForest(shot_made_bool~.-action_type-shot_made_flag,data=kobe_frame, subset=train_id, ntree=ntrees[i], mtry=13, importance=TRUE)
  preds = predict(bag, newdata=kobe[-train_id,])
  preds = ifelse(preds=="No", 0, 1)
  acc[i] = sum(preds==test_y)/length(test_y) 
}
plot(ntrees, acc, xlab="n° trees", ylab="accuracy", ylim=c(.5,.6), pch=20, col="blue")
```


### 3.1 Random forest
Random forest is a generalization of the bagging technique, which aims to further reduce the variance of models based on decision trees. In this case, besides sampling many datasets and building a different tree on each dataset, the algorithm considers only a limited number of attributes to, which are chosen randomly at each split step. This helps to reduce the correlation between trees, which tend to consider some major attributes at the beginning and hence deliver similar results.\
The number of variables taken into account is a hyperparameter, which is normally $\sqrt{p}$ in case of $p$ predictors. In the following plot are displayed the validation accuracies for models with different values of $p$. As you may have guessed, a model which considers all 13 predictors coincides with the bagging model illustrated in the last section.\
As we can see, the highest accuracy is reached with as little as two predictors and is comparable with the one of the simple decision tree model. This may also explain why the bagging model resulted in a poorer performance.
```{r}
set.seed(0)
acc = rep(NA, 9)
attr = 2:10
for(i in 1:9) {
  forest = randomForest(shot_made_bool~.-action_type-shot_made_flag,data=kobe_frame, subset=train_id, mtry=attr[i], importance=TRUE)
  preds = predict(forest, newdata=kobe[-train_id,])
  preds = ifelse(preds=="No", 0, 1)
  acc[i] = sum(preds==test_y)/length(test_y) 
}
plot(attr, acc, xlab="mtry", ylab="accuracy", ylim=c(.5,.65), pch=20, col="blue")
```


### 4. Support Vector Machines
In this section we will consider another popular classification technique, that is Support Vector Machines (Svm). The goal of this technique is to find a vector *w*, called *support vectors*, that satisfies the equation 

<center>$y(w*x-b)>=1$</center>

for every training sample *x*. In order to do so an iterative algorithm is applied, and *w* is computed as a weighted sum of all the training samples. In case the hyperplane in which the training samples are placed is not linearly separable (i.e. there is no vector *w* which satisfy the equation) it is possible to introduce a so-called slack variable in order to relax the constraint.\
In order to account for categorical attributes it is possible to use the same approach seen in the logistic regression, i.e. a set of dummy variables that are able to encode all the values of the attribute domain.

<!-- Try als polynomial and sigmoid-->
```{r}
library(e1071)
dat = kobe_frame[train_id,]
acc = rep(NA, 4)
ker = c("linear", "polynomial", "radial", "sigmoid")
for (i in 1:4) {
  svmlin = svm(shot_made_bool~., data=dat, kernel=ker[i], scale=TRUE)
  preds = predict(svmlin, kobe_frame[-train_id,])
  preds = ifelse(preds=="No", 0, 1)
  acc[i] = sum(preds==test_y)/length(test_y)
}
acc
```

### 6. K-nearest Neighbors
The last classification technique we present here is K-nearest Neighbors, a.k.a. Knn. The idea behind this algorithm is to assign samples which are "near" to each other to the same class. Much like decision trees, despite its intuitive nature this method performs surprisingly well in many real cases and is appreciated for its high degree of interpretability, which makes its results easier to understand than e.g. logistic regression or support vector machines.\
Given that, the main disadvantage of Knn is that it is computationally slow at testing time: in fact, the dataset has to be traversed each time in order to find the "nearest" samples and to produce a prediction. This also that the storage requirements of this technique are quite disfavorable, as well, since it requires to store the whole dataset and to keep it available at test time. This characteristics are radically different from the ones shown by Svm, which only requires to store the support vector and is able to produce a prediction just by multiplying that vector with the input.\
The central point when dealing with a Knn classifier is how to define the distance between to samples. The typical approach is here to use some kind of Minkowski distance (most commonly the Euclidean or the Manhattan distance) in the predictor hyperspace. However there are at least two aspects that must be considered. First of all, the attributes normally have big differences in their domain range. Some attributes like loc_x span an interval of several hundreds, while others are contained within just two or three numbers. In order to prevent a subset of predictors from determining the outcome alone, the attributes have to be normalized according to their mean and their standard distance.\
Secondly, as the number of dimensions in a hyperspace increases the difference between the neighrest and the furthest sample reduces. In a high-dimensional space distance metrics lose any significance and the performance of the model is seriously harmed.\
The most important issue in our case is that it is particularly difficult to define the distance in case of categorical attributes. If the variable is ordinal, it is still possible to assign an incremental value to each domain level. Dummy variables here are pointless, since they increase drastically the number of dimensions without providing a real advantage.\
Keeping this considerations in mind we build a Knn classifier that only considers 6 of the original 13 attributes, i.e. season, feet_x, feet_y, shot_distance, shot_angle and shot_type. Knn models must be configured with a hyperparameter *k* that indicates the number of samples to be consider. In fact, the prediction is produced as a majority vote among the *k* nearest neighbors - hence the name - that can be possibly weighted to account for the distance to each neighbor.\
The following plot shows the validation accuracy of multiple models with values of *k* ranging between 3 and 99. At the beginning the performance improves along with the number of neighbors considered, then it reaches a plateau at roughly 60% accuracy, after which there is little or no improvement. For a good bias-variance tradeoff it would be therefore dvisable to choose a value of *k* near to the rightmost knee of the curve, that in this case is k=43.

```{r}
library(class)
set.seed(1)
shot_type_num = ifelse(shot_type=="3PT Field Goal", 1, 0)
train.X = cbind(season, feet_x, feet_y, shot_distance, shot_angle, shot_type_num)[train_id,]
train.Y = shot_made_flag[train_id]
test.X = cbind(season, feet_x, feet_y, shot_distance, shot_angle, shot_type_num)[-train_id,]
neighbors = seq(3,99,2)
acc = rep(NA, length(neighbors))
for (i in seq(1, length(neighbors))) {
  preds = knn(train.X, test.X, train.Y, k=neighbors[i])
  table(preds, test_y)
  acc[i] = sum(preds==test_y)/length(test_y)
}
plot(neighbors, acc, xlab="k", ylab="accuracy", ylim=c(.5,.65), pch=20, col="blue")
```

### 7. Dimensionality reduction

##### 7.1 Principal Component Analysis
The most popular technique for dimensionality reduction is probably Principal Components Analysis (Pca). Intuitively, this method wants to perform a change of basis in the predictor hyperplane, choosing each basis vector according to its capability to explain the variance in the data.\
This is most commonly done by means of an iterative algorithm that chooses at each step the vector that leads to the most variance when the original data are projected onto it. For this reason the vectors are taken into the basis in descending order of importance, since the last ones only cover the variance that has been left unexplained by the first ones. The values in the change of basis matrix give then a hint about which of the original attributes plays a significant role in characterizing the data.\
Unfortunately, since the basis vectors are linear dependencies of the original attributes it is important that the latters take on numerical value: therefore, just like in the case of Knn is not possible to use dummy variables to encode the categorical attributes. Our analysis will thus consist in performing a Pca analysis on the continuous attributs we have used also for Knn (i.e. season, feet_x, feet_y, shot_distance, shot_angle and shot_type) and training a logistic regression model which the first two principal components (i.e. basis vectors) along with the categorical attributes.

```{r}
pr.out = prcomp(kobe, scale=TRUE)
```

#### 7.2 Factor Analysis of Mixed Data
In this section we have a brief look at another dimensionality reduction technique, called Factor Analysis of Mixed Data (FAMD). Its goal is to 

```{r}
library("FactoMineR")
library("factoextra")
famd = FAMD(kobe[,-which(names(kobe)=="shot_made_flag")], graph = FALSE)
get_eigenvalue(famd)
fviz_screeplot(famd)
var = get_famd_var(famd)
var$contrib
fviz_famd_var(famd, repel = TRUE)
```


### (7. Evaluation metrics)
<!--
hyperparameters (logistic regression threshold)
other metrics (precision, recall, roc curve)
-->

<!--
Compare all models
-->